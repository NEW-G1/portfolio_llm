{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqK0E0S_lOJG",
        "outputId": "ea34ac1b-db8e-4b7b-98ae-7150e3dd113f"
      },
      "outputs": [],
      "source": [
        "!pip install pycoingecko requests tiktoken cohere openai langchain langchain_experimental google-search-results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIB6YLzGHh0g"
      },
      "source": [
        "###1.IMPORT LIBRARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csnX63HhWwyR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "from langchain.document_loaders.base import Document\n",
        "from langchain.utilities import ApifyWrapper\n",
        "from langchain.vectorstores import Vectara\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.prompts import PromptTemplate,ChatPromptTemplate, MessagesPlaceholder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n520zBIcHwhS"
      },
      "source": [
        "###2.KEY SETTINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HR1H9tt4KYh"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"]      = \"OPENAI_API_KEY\"\n",
        "os.environ[\"VECTARA_CUSTOMER_ID\"] = \"VECTARA_CUSTOMER_ID\"\n",
        "os.environ[\"VECTARA_CORPUS_ID\"]   = \"VECTARA_CORPUS_ID\"\n",
        "os.environ[\"VECTARA_API_KEY\"]     = \"VECTARA_API_KEY\"\n",
        "os.environ[\"GOOGLE_API_KEY\"]      = \"GOOGLE_API_KEY\"\n",
        "os.environ[\"GOOGLE_CSE_ID\"]       = \"GOOGLE_CSE_ID\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uZcWRzoDY78"
      },
      "source": [
        "###3.INITIALIZE VECTARA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBoARJU1DTNW"
      },
      "outputs": [],
      "source": [
        "vectara = Vectara(\n",
        "    vectara_customer_id = os.getenv(\"VECTARA_CUSTOMER_ID\"),\n",
        "    vectara_corpus_id   = os.getenv(\"VECTARA_CORPUS_ID\"),\n",
        "    vectara_api_key     = os.getenv(\"VECTARA_API_KEY\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3xfky6-x6YH"
      },
      "outputs": [],
      "source": [
        "# ChatGPT 모델 지정\n",
        "#llm = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=0)\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo-1106',temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N4PU03Zik8j"
      },
      "outputs": [],
      "source": [
        "# Create Prompt\n",
        "question_prompt_template = \"\"\"Use the following portion of a long document to see if any of the text is relevant to answer the question.\n",
        "Return any relevant text verbatim.\n",
        "{context}\n",
        "Question: {question}\n",
        "Relevant text, if any:\"\"\"\n",
        "#prompt = ChatPromptTemplate.from_template(question_prompt_template)\n",
        "QUESTION_PROMPT = PromptTemplate(\n",
        "    template=question_prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "#chain_type_kwargs = {\"verbose\": True,  \"question_prompt\": QUESTION_PROMPT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXJFFnFlKio_"
      },
      "outputs": [],
      "source": [
        "# Define retriever\n",
        "retriever = vectara.as_retriever(lambda_val=0.025, k=5, filter=None)\n",
        "\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "knowledgeBase_qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    input_key=\"query\",\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": QUESTION_PROMPT\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgxCZvXaDyGQ"
      },
      "outputs": [],
      "source": [
        "query = \"지식기반 기준으로 답해주세요.시스템반도체산업 현황 및 전망\"\n",
        "response = knowledgeBase_qa({\"query\": query},return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XHd-qS4m-op"
      },
      "outputs": [],
      "source": [
        "source_documents = response['source_documents']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iTrNrjxrihJ"
      },
      "source": [
        "# 결과...\n",
        "## LLM Result\n",
        "## Metadata - 포멧 이쁘게 해서 보여주세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV2oEZxYo0fj",
        "outputId": "1ce159fa-d0fd-4476-cc8c-8915e180e128"
      },
      "outputs": [],
      "source": [
        "print(response['result'])\n",
        "\n",
        "for i in range(len(source_documents)):\n",
        "  #print(source_documents[i].page_content[0:100])\n",
        "  print(source_documents[i].metadata)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
